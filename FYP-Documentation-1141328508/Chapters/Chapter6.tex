\chapter{Results and Findings} 
% Main chapter title

\label{Chapter6} 
%Call reference to this chapter use \ref{ChapterX}

\lhead{Chapter 6. \emph{Results and Findings}} 
% Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

\doublespacing
% LINE FORMATTING

%\clearpage
%\pagebreak

% MAIN SECTION ==============================

\section{Phase 1}

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1.5ex]
	
	\item \textbf{Data validation.}
	This activity has been successfully achieved. It has been found the method can detect unmatched numbers of commas, unsuitable data types during data importation from CSV to PostgreSQL database and identify the uniqueness of rows and columns in data. Results and detailed information is provided in Appendix B.2 to B.4.
	
	\item \textbf{Golang programming for import CSV files into PostgreSQL database.}
	This activity has been successfully achieved. The program is capable to read 100 rows of data from three datasets and import into PostgreSQL database. Results and detailed information is provided in Appendix H.
	
	\pagebreak
	
	\item \textbf{Sequential and concurrent programming with Golang on PostgreSQL database retrieval.}
	This activity has been successfully achieved. The program is capable to prove concurrent processing is faster than sequential in data retrieval with PostgreSQL database. Results and detailed information is provided in Appendix G.
	
	\item \textbf{Sequential and concurrent programming with Golang on reading CSV files.}
	This activity has been successfully achieved. The program is capable to prove concurrent processing is faster than sequential in reading CSV data. Results and detailed information is provided in Appendix F.
	
\end{enumerate}

\section{Phase 2}

\begin{enumerate}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1.5ex]
	
	\item \textbf{Data encoding.} This activity has been successfully achieved. The dirty and corrupted CSV raw datasets can be converted into consistent format with stream editor. Result and detailed information is provided in Appendix J.3.
	
	\item \textbf{Development of PL/pgSQL scripts for data transformation.} This activity has been successfully achieved. The developed scripts is capable to extract data from CSV format from raw datasets and import into PostgreSQL database. Result and detailed information is provided in Appendix K.3.
	
	\item \textbf{Development of Go and Rust Object Relational Mapping (ORM) program for data retrieval.} This activity has been successfully achieved. The Go and Rust program developed is capable to retrieved data from CSV file and PostgreSQL database and map into object model in sequential and concurrent manner. The activity proves concurrent processing is faster than sequential in data retrieval with PostgreSQL database and reading CSV data. Moreover, it proves Go programming languages possess faster processing time compared to Rust programming languages. Result and detailed information is provided in Appendix Q.
	
	\item \textbf{Development of PL/pgSQL DDL scripts for normalized entity creation.} This activity has been successfully achieved. The database design is capable to define table and establish relationship between entity. In addition, the PL/pgSQL's DDL scripts developed is able to create database entity based on the database design correctly. Result and detailed information is provided in Appendix M.4.
	
	\item \textbf{Development of Go data parser program.} This activity has been successfully achieved. The developed Go program is capable to eliminate NULL values and standardize the records of specific columns to promote conformity and usability of data. Result and detailed information is provided in Appendix N.2. 
	
	\item \textbf{Database Tuning.} This activity has been successfully achieved. The number of database maximum connections, amount of shared buffer utilized and maximum of shared memory segments are configured to increase performance and transaction efficiency of concurrent program. Result and detailed information is provided in Appendix O. 
	
	\item \textbf{Development of PL/pgSQL's DML scripts and Go concurrent program for database migration.} This activity has been successfully achieved. The PL/pgSQL's DML, DCL scripts is capable to retrieve unique data from legacy storage and insert into normalize table. In addition, the scripts and Go program are capable to migrate more than 4 millions row of data into normalized table without causing missing of records. Other than that, the size of database is reduced and all records are correct after the migration. Result and detailed information is provided in Appendix P.3 and P.4. 
	
\end{enumerate}

